{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from typing import List, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from MeCab import Tagger\n",
    "import unidic_lite\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "\n",
    "from racoon.dataset import TableDataset\n",
    "from racoon.encoder import LabelEncoder, TargetEncoder, CountEncoder\n",
    "from racoon.runner import ModelSet, BaseRunner, StackedRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tagger = Tagger(f'-Owakati -r /dev/null -d {unidic_lite.DICDIR}')\n",
    "        \n",
    "    def __call__(self, text:str) -> List[str]:\n",
    "        return self.tagger.parse(text)\n",
    "    \n",
    "classical_tokenizer = ClassicalTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jablogs.com/detail/43132\n",
    "from math import cos, asin, sqrt\n",
    "def closest(station_name, data, v, n=3):\n",
    "    def distance(lat1, lon1, lat2, lon2):\n",
    "        p = 0.017453292519943295\n",
    "        hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n",
    "        return 12742 * asin(sqrt(hav))\n",
    "\n",
    "    dist = list(map(lambda p: distance(v['latitude'],v['longitude'],p['latitude'],p['longitude']), data))\n",
    "    indices = np.argsort(dist)\n",
    "    results = []\n",
    "    # 名前が重複している可能性がある(例：新宿)\n",
    "    for ind in indices:\n",
    "        if station_name[ind] not in results:\n",
    "            results.append(station_name[ind])\n",
    "            if len(results) >= n:\n",
    "                break\n",
    "\n",
    "    return results, geodesic((data[indices[0]][\"latitude\"], data[indices[0]][\"longitude\"]), (v[\"latitude\"], v[\"longitude\"])).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, pred):\n",
    "    return np.sqrt(mean_squared_log_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train_data.csv\")\n",
    "test_df = pd.read_csv(\"../input/test_data.csv\")\n",
    "\n",
    "station_df = pd.read_csv(\"../input/station_list.csv\")\n",
    "station_name = station_df[\"station_name\"].tolist()\n",
    "station_latlon = station_df[[\"longitude\", \"latitude\"]].to_dict(orient=\"records\")\n",
    "\n",
    "whole_df = pd.concat([train_df, test_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(df:pd.DataFrame, cols:List[str], target) -> pd.DataFrame:\n",
    "    train_length = len(target)\n",
    "    encoder = TargetEncoder(cols=cols).fit(X=df.iloc[:train_length][cols], y=target)\n",
    "    output_df = encoder.transform(df[cols])\n",
    "    output_df.columns = [col+\"_te\" for col in output_df.columns]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_encodeing(df:pd.DataFrame, col:str) -> pd.DataFrame:\n",
    "    series = pd.to_datetime(df[col])\n",
    "    output_df = []\n",
    "    \n",
    "    year = series.dt.year.copy()\n",
    "    year.fillna(-1, inplace=True)\n",
    "    year = year.astype(int)\n",
    "\n",
    "    year.name = col + \"_year\"\n",
    "    \n",
    "    month = series.dt.month.copy()\n",
    "    month.name = col + \"_month\"\n",
    "    month.fillna(-1, inplace=True)\n",
    "    month = month.astype(int)\n",
    "\n",
    "    day = series.dt.day.copy()\n",
    "    day.name = col + \"_day\"\n",
    "    day.fillna(-1, inplace=True)\n",
    "    day = day.astype(int)\n",
    "\n",
    "    return pd.concat([year, month, day], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoding(df:pd.DataFrame, decompose_func:Callable, col=\"name\", model_name:str=\"paraphrase-multilingual-MiniLM-L12-v2\", lower:bool=True) -> pd.DataFrame:\n",
    "    model_list = {\n",
    "        \"paraphrase-multilingual-MiniLM-L12-v2\": \"phm-mini-lm-l12-v2\",\n",
    "        \"bert-base-multilingual-uncased\": \"bert-base-multi-uncased\",\n",
    "        \"xlm-roberta-large\": \"xlm-roberta-large\",\n",
    "    }\n",
    "\n",
    "    batch_size=16\n",
    "    texts = df[col].tolist()\n",
    "    \n",
    "    \n",
    "    if lower:\n",
    "        texts = list(map(str.lower, texts))\n",
    "    \n",
    "    if model_name == \"paraphrase-multilingual-MiniLM-L12-v2\":\n",
    "        \n",
    "        model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", cache_folder=\"../input/model_cache/\")\n",
    "        encoded: SentenceTransformer = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "        \n",
    "    elif model_name in [\"bert-base-multilingual-uncased\", \"xlm-roberta-large\"]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"../input/model_cache/\")\n",
    "        model = AutoModel.from_pretrained(model_name, cache_dir=\"../input/model_cache/\")\n",
    "        \n",
    "        target_device = torch.device(device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(target_device)\n",
    "        \n",
    "        encoded = np.zeros((len(texts), model.config.hidden_size), dtype=np.float32)\n",
    "        for batch_idx in tqdm(range(0, len(texts), batch_size), desc=\"Batching\"):\n",
    "            inputs = tokenizer(texts[batch_idx:batch_idx+batch_size], max_length=98, padding=\"max_length\", return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                inputs = {k:v.to(target_device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                # cls token or mean_pooling\n",
    "                outputs = outputs[\"last_hidden_state\"][:, 0, :].cpu()\n",
    "                encoded[batch_idx:batch_idx+batch_size, :] = outputs.numpy()\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "          \n",
    "    output_df = pd.DataFrame(decompose_func.fit_transform(encoded))\n",
    "    output_df.columns = [f\"{model_list[model_name]}_{decompose_func.__class__.__name__}_{i}\" for i in range(output_df.shape[1])]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_encoding(df:pd.DataFrame, decompose_func:Callable, tokenizer=None, col=\"name\"):\n",
    "\n",
    "    temp = df[col].str.lower()\n",
    "\n",
    "    pipline = make_pipeline(\n",
    "        TfidfVectorizer(tokenizer=tokenizer, min_df=2, ),\n",
    "        decompose_func,\n",
    "    )\n",
    "\n",
    "    output_df = pipline.fit_transform(temp)\n",
    "    output_df = pd.DataFrame(output_df)\n",
    "    output_df.columns = [f\"tfidf_{decompose_func.__class__.__name__}_{i}\" for i in range(output_df.shape[1])]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encoding(df:pd.DataFrame, cols:List[str]) -> pd.DataFrame:\n",
    "    output_df = CountEncoder(cols=cols).fit_transform(df[cols])\n",
    "    output_df.columns = [col+\"_ce\" for col in output_df.columns]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(df:pd.DataFrame, base_latitude:int, base_longitude:int, base_name:str) -> pd.DataFrame:\n",
    "    \"\"\"東京: 35.681753 139.764708 \"\"\"\n",
    "    latitudes = df[\"latitude\"]\n",
    "    longitudes = df[\"longitude\"]\n",
    "    distances = []\n",
    "    for latitude, longitude in zip(latitudes, longitudes):\n",
    "        distances.append(geodesic((base_latitude, base_longitude), (latitude, longitude)).km)\n",
    "    \n",
    "    distances = pd.DataFrame(distances)\n",
    "    distances.columns = [f\"dist_{base_name}\"]\n",
    "    return distances\n",
    "\n",
    "def nearest_station_to_name(df:pd.DataFrame, nearest_station_cols:List[str]) -> pd.Series:\n",
    "    nearest_stations = df[nearest_station_cols].apply(lambda x: \",\".join(x), axis=1)\n",
    "    df[\"name\"] = nearest_stations + \",\" + df[\"name\"]\n",
    "    return df[\"name\"]\n",
    "\n",
    "def gen_nearest_station(df:pd.DataFrame, station_name, station_latlon) -> pd.DataFrame:\n",
    "    latlons = df[[\"latitude\", \"longitude\"]].to_dict(orient=\"records\")\n",
    "    nearest_station = list(map(lambda data: closest(station_name, station_latlon, data), latlons))\n",
    "\n",
    "    nearest_station_name = [row[0] for row in nearest_station]\n",
    "    nearest_station_name = pd.DataFrame(nearest_station_name)\n",
    "    nearest_station_name.columns = [f\"nearest_station_{i}\" for i in range(nearest_station_name.shape[1])]\n",
    "\n",
    "    nearest_station_dist = [row[1] for row in nearest_station]\n",
    "    nearest_station_dist = pd.DataFrame({\"nearest_station_dist\": nearest_station_dist})\n",
    "\n",
    "    output_df = pd.concat([nearest_station_name, nearest_station_dist], axis=1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_encoding(df: pd.DataFrame, pk: List[str], agg_funcs, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    simple groupby\n",
    "    examples\n",
    "    ========\n",
    "    |    | city   | cat   |   target |\n",
    "    |---:|:-------|:------|---------:|\n",
    "    |  0 | tokyo  | A     |        0 |\n",
    "    |  1 | nagoya | B     |        1 |\n",
    "    |  2 | osaka  | A     |        0 |\n",
    "    |  3 | tokyo  | B     |        1 |\n",
    "    |  4 | nagoya | A     |        0 |\n",
    "    |  5 | osaka  | C     |        1 |\n",
    "    |  6 | tokyo  | A     |        0 |\n",
    "    |  7 | osaka  | C     |        1 |\n",
    "    |  8 | tokyo  | A     |        0 |\n",
    "    aggregator.generate_statics_features(df, [\"city\"], {\"target\":[\"count\"]})\n",
    "    |    | city   |   cat_count |\n",
    "    |---:|:-------|------------:|\n",
    "    |  0 | nagoya |           2 |\n",
    "    |  1 | osaka  |           3 |\n",
    "    |  2 | tokyo  |           4 |\n",
    "    \"\"\"\n",
    "    \n",
    "    agg_pvs = df.groupby(pk).agg(agg_funcs)\n",
    "        \n",
    "    rename_columns = ['_'.join(col).strip() for col in agg_pvs.columns.values]\n",
    "    \n",
    "    if suffix != \"\":\n",
    "        rename_columns = [suffix+col for col in rename_columns]\n",
    "    else:\n",
    "        suffix = \"/\".join(pk)\n",
    "        rename_columns = [suffix+\"_\"+col for col in rename_columns]\n",
    "        \n",
    "    agg_pvs.columns = rename_columns\n",
    "    \n",
    "    agg_pvs.reset_index(inplace=True)\n",
    "\n",
    "    return pd.merge(df, agg_pvs, on=pk, how=\"left\")[rename_columns]\n",
    "\n",
    "def math_encoding(df:pd.DataFrame, cols:List[str]) -> pd.DataFrame:\n",
    "    output_df = []\n",
    "    for col in cols:\n",
    "        enc = df[col]**2\n",
    "        enc.name = col + \"**2\"\n",
    "        output_df.append(enc)\n",
    "\n",
    "    return pd.concat(output_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df[[\"number_of_reviews\", \"reviews_per_month\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df:pd.DataFrame, columns=List[str]) -> pd.DataFrame:\n",
    "    return LabelEncoder(output_suffix=\"\").fit_transform(df[columns])\n",
    "\n",
    "def make_features(df:pd.DataFrame):\n",
    "\n",
    "    # preprocess\n",
    "    _df = df.copy()\n",
    "    _df[\"latlon_cluster\"] = KMeans(n_clusters=100, random_state=42).fit_predict(_df[[\"latitude\",\"longitude\"]])\n",
    "    # _df = pd.concat([_df, gen_nearest_station(_df, station_name=station_name, station_latlon=station_latlon)], axis=1)\n",
    "    # _df[\"name\"] = nearest_station_to_name(_df, nearest_station_cols=[\"nearest_station_0\",\"nearest_station_1\",\"nearest_station_2\"])\n",
    "\n",
    "    target = _df[_df[\"y\"].notnull()][\"y\"].values\n",
    "\n",
    "    output_df = []\n",
    "\n",
    "    output_df.append(_df[[\"number_of_reviews\", \"minimum_nights\", \"availability_365\", \"reviews_per_month\", \"latlon_cluster\", \"latitude\", \"longitude\"]])\n",
    "\n",
    "    enc_funcs = [\n",
    "        [label_encoding, {\"df\":_df, \"columns\":[\"neighbourhood\", \"room_type\"]}],\n",
    "        [count_encoding, {\"df\":_df, \"cols\":[\"neighbourhood\", \"minimum_nights\"]}],\n",
    "        [datetime_encodeing, {\"df\":_df, \"col\":\"last_review\"}],\n",
    "        [compute_distance, {\"df\":_df, \"base_latitude\":35.681753, \"base_longitude\":139.764708, \"base_name\":\"tokyo\"}],\n",
    "        [compute_distance, {\"df\":_df, \"base_latitude\":35.688690, \"base_longitude\":139.698812, \"base_name\":\"shinjuku\"}],\n",
    "        [compute_distance, {\"df\":_df, \"base_latitude\":35.658700, \"base_longitude\":139.700872, \"base_name\":\"shibuya\"}],\n",
    "        [compute_distance, {\"df\":_df, \"base_latitude\":35.710430, \"base_longitude\":139.809332, \"base_name\":\"skytree\"}],\n",
    "\n",
    "        [agg_encoding, {\"df\":_df,\"pk\":[\"neighbourhood\"], \"agg_funcs\":{\n",
    "            \"minimum_nights\":[\"mean\", \"std\"],\n",
    "            \"availability_365\":[\"mean\", \"std\"],\n",
    "            \"number_of_reviews\":[\"mean\", \"max\", \"std\"],\n",
    "            }}],\n",
    "\n",
    "        # [agg_encoding, {\"df\":_df,\"pk\":[\"room_type\"], \"agg_funcs\":{\n",
    "        #     \"minimum_nights\":[\"mean\", \"std\"],\n",
    "        #     \"availability_365\":[\"mean\", \"std\"],\n",
    "        #     \"number_of_reviews\":[\"mean\", \"max\", \"std\"],\n",
    "        #     }}],\n",
    "\n",
    "        [agg_encoding, {\"df\":_df,\"pk\":[\"neighbourhood\", \"room_type\"], \"agg_funcs\":{\n",
    "            \"minimum_nights\":[\"mean\", \"std\"],\n",
    "            \"availability_365\":[\"mean\", \"std\"],\n",
    "            \"number_of_reviews\":[\"mean\", \"max\", \"std\"],\n",
    "            }}],\n",
    "\n",
    "        # [agg_encoding, {\"df\":_df,\"pk\":[\"latlon_cluster\"], \"agg_funcs\":{\n",
    "        #     \"minimum_nights\":[\"mean\", \"std\"],\n",
    "        #     \"availability_365\":[\"mean\", \"std\"],\n",
    "        #     \"number_of_reviews\":[\"mean\", \"max\", \"std\"],\n",
    "        #     }}],\n",
    "        \n",
    "        [math_encoding, {\"df\":_df, \"cols\":[\"minimum_nights\", \"availability_365\", \"number_of_reviews\"]}],\n",
    "        # [gen_nearest_station, {\"df\":_df, \"station_name\": station_name, \"station_latlon\": station_latlon}],\n",
    "        # [nearest_station_to_name, {\"df\":_df, \"nearest_station_cols\": [\"nearest_station_0\",\"nearest_station_1\",\"nearest_station_2\"]}],\n",
    "        [target_encoding, {\"df\":_df, \"cols\":[\"neighbourhood\", \"latlon_cluster\"], \"target\":np.log1p(target)}],\n",
    "\n",
    "        [tfidf_encoding, {\"df\":_df, \"decompose_func\": TruncatedSVD(n_components=25, random_state=42)}],\n",
    "        [tfidf_encoding, {\"df\":_df, \"decompose_func\": NMF(n_components=25, random_state=42)}],\n",
    "\n",
    "        [transformer_encoding, {\"df\":_df, \"decompose_func\": PCA(n_components=30, random_state=42), \"col\":\"name\", \"lower\":True, \"model_name\":\"paraphrase-multilingual-MiniLM-L12-v2\"}],\n",
    "        [transformer_encoding, {\"df\":_df, \"decompose_func\": PCA(n_components=30, random_state=42), \"col\":\"name\", \"lower\":True, \"model_name\":\"bert-base-multilingual-uncased\"}],\n",
    "        \n",
    "        # [transformer_encoding, {\"df\":df, \"decompose_func\": TruncatedSVD(n_components=30, random_state=42), \"col\":\"name\", \"lower\":True, \"model_name\":\"bert-base-multilingual-uncased\"}],\n",
    "        # [transformer_encoding, {\"df\":df, \"decompose_func\": PCA(n_components=30, random_state=42), \"col\":\"name\", \"lower\":True, \"model_name\":\"xlm-roberta-large\"}],\n",
    "    ]\n",
    "\n",
    "    for func, params in tqdm(enc_funcs, desc=\"Generate Features...\"):\n",
    "        print(f\"Exec: {func.__name__}\")\n",
    "        output_df.append(func(**params))\n",
    "\n",
    "    output_df = pd.concat(output_df, axis=1)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = make_features(whole_df)\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_df[\"y\"]\n",
    "log_targets = np.log1p(targets)\n",
    "train_features = feature_df.iloc[:len(train_df)].values\n",
    "test_features = feature_df.iloc[len(train_df):].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cv(path:str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = load_cv(\"../input/fold/kfold.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TableDataset(\n",
    "    train_features=train_features,\n",
    "    train_targets=log_targets,\n",
    "    test_features=test_features,\n",
    "    cv=cv,\n",
    "    type_of_target=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_set = ModelSet(\n",
    "#     model=RandomForestRegressor(n_estimators=1000, max_depth=4)\n",
    "# )\n",
    "# model_set\n",
    "\n",
    "model_set_lgb = ModelSet(\n",
    "    model=lgbm.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        n_estimators=10000,\n",
    "        max_depth=8,\n",
    "        importance_type=\"gain\",\n",
    "        random_state=42,\n",
    "        colsample_bytree=0.4,\n",
    "        subsamples=0.7,\n",
    "        # subsample_freq=3,\n",
    "    ),\n",
    "    fit_params={\n",
    "        \"callbacks\": [\n",
    "            lgbm.early_stopping(stopping_rounds=200),\n",
    "            lgbm.log_evaluation(period=200),\n",
    "            ]\n",
    "        }\n",
    ")\n",
    "\n",
    "model_set_xgb = ModelSet(\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=10000,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        verbosity=0,\n",
    "        random_state=42,\n",
    "        importance_type=\"gain\",\n",
    "    ),\n",
    "    fit_params={\n",
    "        \"callbacks\":[\n",
    "            # xgb.callback.EarlyStopping(rounds=50),\n",
    "        ],\n",
    "        \"early_stopping_rounds\":100,\n",
    "        \"verbose\":False,\n",
    "    }\n",
    ")\n",
    "\n",
    "model_set_cat = ModelSet(\n",
    "    model = cbt.CatBoostRegressor(\n",
    "        iterations=100000,\n",
    "        loss_function='RMSE',\n",
    "        use_best_model=True,\n",
    "        random_seed=42,\n",
    "        learning_rate=0.1,\n",
    "        verbose=500,\n",
    "    ),\n",
    "    fit_params={\n",
    "        \"early_stopping_rounds\":200,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_runner = StackedRunner(\n",
    "    runner=BaseRunner(\n",
    "        table_dataset=ds,\n",
    "        metric_func=mean_squared_error,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_runner.train_eval([\n",
    "    model_set_lgb,\n",
    "    # model_set_xgb,\n",
    "    model_set_cat,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_runner.eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{rmsle(targets, np.expm1((stacked_runner.eval_result[2].oof))):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{rmsle(targets, np.expm1(((stacked_runner.eval_result[0].oof+stacked_runner.eval_result[2].oof)/2))):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/exp007/oof_val_pred.pkl\", \"wb\") as f:\n",
    "    preds = np.array([stacked_runner.eval_result[0].oof, stacked_runner.eval_result[1].oof, stacked_runner.eval_result[2].oof]).T\n",
    "    preds = np.expm1(preds)\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/exp007/oof_test_pred.pkl\", \"wb\") as f:\n",
    "    preds = np.array([stacked_runner.eval_result[0].test_probas, stacked_runner.eval_result[1].test_probas, stacked_runner.eval_result[2].test_probas]).T\n",
    "    preds = np.expm1(preds)\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emsamble_preds = np.array([\n",
    "#     stacked_runner.eval_result[0].oof,\n",
    "#     stacked_runner.eval_result[1].oof,\n",
    "#     stacked_runner.eval_result[2].oof,\n",
    "# ]).mean(axis=0)\n",
    "\n",
    "# rmsle(targets, np.expm1(emsamble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = np.array([t.feature_importances_ for t in stacked_runner.train_models[0]])\n",
    "def plot_importance(importances:np.ndarray, col_name:List[str]):\n",
    "    importance_df = pd.DataFrame(importances)\n",
    "    importance_df.columns = col_name\n",
    "\n",
    "    sort_col = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    temp = pd.melt(importance_df)\n",
    "\n",
    "    plt.figure(figsize = (8,10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.boxplot(\n",
    "        data=temp,\n",
    "        x=\"value\",\n",
    "        y=\"variable\",\n",
    "        order=sort_col[:50],\n",
    "    )\n",
    "\n",
    "plot_importance(importances, feature_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.expm1((stacked_runner.eval_result[0].test_probas+stacked_runner.eval_result[1].test_probas)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"../input/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['y'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"../output/watanabe_exp007_k.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f366a9e4e0794749bfca66f4e95c375d172aea087d6d4cf16369874737853a43"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
